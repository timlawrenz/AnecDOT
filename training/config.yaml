# QLoRA Training Configuration for AnecDOT

# Model settings
model_name: "google/gemma-2b-it"  # 2B instruction-tuned model
load_in_4bit: true
bnb_4bit_compute_dtype: "bfloat16"
bnb_4bit_quant_type: "nf4"

# LoRA settings
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "v_proj"
  - "k_proj"
  - "o_proj"

# Training hyperparameters
learning_rate: 2.0e-4
num_train_epochs: 5
per_device_train_batch_size: 4
gradient_accumulation_steps: 4  # Effective batch size: 16
warmup_ratio: 0.1
lr_scheduler_type: "cosine"
weight_decay: 0.01
max_grad_norm: 1.0

# Data settings
max_seq_length: 512
train_val_split: 0.9
seed: 42

# Logging and checkpointing
output_dir: "./training/outputs"
logging_steps: 10
save_steps: 50
save_total_limit: 3
evaluation_strategy: "no"  # Disabled to avoid quantization errors during training
eval_steps: 50

# Evaluation
generate_samples_during_eval: true
num_eval_samples: 3
